{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-27T16:49:52.074313Z",
     "iopub.status.busy": "2025-11-27T16:49:52.073986Z",
     "iopub.status.idle": "2025-11-27T16:51:15.418567Z",
     "shell.execute_reply": "2025-11-27T16:51:15.417721Z",
     "shell.execute_reply.started": "2025-11-27T16:49:52.074288Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# With the toggle '+ Add Input' on the right, add the covid19-radiography-database dataset\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:51:15.420174Z",
     "iopub.status.busy": "2025-11-27T16:51:15.419827Z",
     "iopub.status.idle": "2025-11-27T16:52:33.067036Z",
     "shell.execute_reply": "2025-11-27T16:52:33.066263Z",
     "shell.execute_reply.started": "2025-11-27T16:51:15.420157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:52:33.068327Z",
     "iopub.status.busy": "2025-11-27T16:52:33.068054Z",
     "iopub.status.idle": "2025-11-27T16:52:33.41271Z",
     "shell.execute_reply": "2025-11-27T16:52:33.411971Z",
     "shell.execute_reply.started": "2025-11-27T16:52:33.068303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# First define your BASE_DIR: this will be the base directory\n",
    "# in wich you will save your final model\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Since we are using Kaggle, define the correct path to problem data\n",
    "dataset_path = Path(\"/kaggle/input/covid19-radiography-database\")\n",
    "dataset_folder = \"COVID-19_Radiography_Dataset\"\n",
    "\n",
    "# Check how the directory is actually organized...\n",
    "image_dir_names = #CODE\n",
    "\n",
    "# ...and finally extract data, filling dataset_files\n",
    "dataset_files = []\n",
    "\n",
    "#CODE\n",
    "#CODE\n",
    "#CODE\n",
    "#CODE\n",
    "\n",
    "\n",
    "# How many images do we have?\n",
    "print(#CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:52:33.414197Z",
     "iopub.status.busy": "2025-11-27T16:52:33.413989Z",
     "iopub.status.idle": "2025-11-27T16:52:34.450921Z",
     "shell.execute_reply": "2025-11-27T16:52:34.450291Z",
     "shell.execute_reply.started": "2025-11-27T16:52:33.414179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Reduce the dimension of the dataset to work faster\n",
    "new_size = #CODE\n",
    "dataset_files = #CODE\n",
    "\n",
    "# How many images do we have in the end?\n",
    "print(#CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check shape of dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T16:54:18.708032Z",
     "iopub.status.busy": "2025-11-27T16:54:18.707534Z",
     "iopub.status.idle": "2025-11-27T16:54:18.71966Z",
     "shell.execute_reply": "2025-11-27T16:54:18.719039Z",
     "shell.execute_reply.started": "2025-11-27T16:54:18.708007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Before proceding, check whether images and masks have the same size\n",
    "\n",
    "img = #CODE\n",
    "mask = #CODE\n",
    "\n",
    "print(f\"image={img.size}, mask={mask.size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plot image and mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:47:33.779543Z",
     "iopub.status.busy": "2025-11-27T10:47:33.779225Z",
     "iopub.status.idle": "2025-11-27T10:47:34.114165Z",
     "shell.execute_reply": "2025-11-27T10:47:34.113513Z",
     "shell.execute_reply.started": "2025-11-27T10:47:33.779523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Before developing any model, plot the images and masks\n",
    "\n",
    "img = #CODE\n",
    "mask = #CODE\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, implement the Dataset and DataLoader classes, which you will use along the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:47:34.121259Z",
     "iopub.status.busy": "2025-11-27T10:47:34.121Z",
     "iopub.status.idle": "2025-11-27T10:47:35.770817Z",
     "shell.execute_reply": "2025-11-27T10:47:35.770249Z",
     "shell.execute_reply.started": "2025-11-27T10:47:34.121236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# This will be your Dataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    # Initialization of the Dataset\n",
    "    def __init__(self, data, transform=None):\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "    def __len__(self):\n",
    "        #CODE\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T15:17:33.484297Z",
     "iopub.status.busy": "2025-11-28T15:17:33.484064Z",
     "iopub.status.idle": "2025-11-28T15:17:33.489935Z",
     "shell.execute_reply": "2025-11-28T15:17:33.489084Z",
     "shell.execute_reply.started": "2025-11-28T15:17:33.484278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This will be your DataLoader class.\n",
    "# (Actually, we will use the MONAI loader)\n",
    "import random\n",
    "\n",
    "class ToyDataLoader:\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.indices = #CODE -> set the indices starting from dataset\n",
    "\n",
    "        #CODE -> shuffle the indices, if necessary\n",
    "\n",
    "        self.current = #CODE\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if #CODE:\n",
    "            raise StopIteration\n",
    "\n",
    "        batch_indices = #CODE\n",
    "        self.current += #CODE\n",
    "\n",
    "        batch = #CODE\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:47:35.778828Z",
     "iopub.status.busy": "2025-11-27T10:47:35.778603Z",
     "iopub.status.idle": "2025-11-27T10:47:35.794041Z",
     "shell.execute_reply": "2025-11-27T10:47:35.793539Z",
     "shell.execute_reply.started": "2025-11-27T10:47:35.778804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Try your dataset and loader\n",
    "ds = SegmentationDataset([10, 20, 30, 40, 50])\n",
    "loader = ToyDataLoader(ds, batch_size=#CODE, shuffle=#CODE)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T15:23:59.487804Z",
     "iopub.status.busy": "2025-11-28T15:23:59.487063Z",
     "iopub.status.idle": "2025-11-28T15:23:59.495205Z",
     "shell.execute_reply": "2025-11-28T15:23:59.494308Z",
     "shell.execute_reply.started": "2025-11-28T15:23:59.487776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check https://monai-dev.readthedocs.io/en/stable/transforms.html\n",
    "from monai.transforms import (\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    ")\n",
    "\n",
    "# Select a size\n",
    "PATCH_SIZE = #CODE\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        # First: load the image in dictionary form\n",
    "        #CODE\n",
    "\n",
    "        # Then, ensure the correct shape -> (W,H)\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        # Now you can use some transformations\n",
    "\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        # And finlly transform the image and labels into a tensor\n",
    "        #CODE\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The same goes for validation transformations:\n",
    "val_transforms = Compose([\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:48:08.289726Z",
     "iopub.status.busy": "2025-11-27T10:48:08.28931Z",
     "iopub.status.idle": "2025-11-27T10:48:08.299166Z",
     "shell.execute_reply": "2025-11-27T10:48:08.298343Z",
     "shell.execute_reply.started": "2025-11-27T10:48:08.289705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "\n",
    "set_determinism(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:48:08.300354Z",
     "iopub.status.busy": "2025-11-27T10:48:08.300024Z",
     "iopub.status.idle": "2025-11-27T10:48:09.370063Z",
     "shell.execute_reply": "2025-11-27T10:48:09.369403Z",
     "shell.execute_reply.started": "2025-11-27T10:48:08.300324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.data import CacheDataset, DataLoader, decollate_batch\n",
    "\n",
    "# Here check if your transforms are working correctly, using your Dataset class and\n",
    "# MONAI's DataLoader\n",
    "check_dataset = #CODE\n",
    "check_loader = #CODE\n",
    "check_data = #CODE\n",
    "image, label = #CODE\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:, :], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:48:09.371231Z",
     "iopub.status.busy": "2025-11-27T10:48:09.37092Z",
     "iopub.status.idle": "2025-11-27T10:48:09.537723Z",
     "shell.execute_reply": "2025-11-27T10:48:09.537081Z",
     "shell.execute_reply.started": "2025-11-27T10:48:09.371207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "import torch\n",
    "\n",
    "# Set the device for the model\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Now we need a NN for 2D images segmentation:\n",
    "# we will use a MONAI UNet -> check https://monai-dev.readthedocs.io/en/fixes-sphinx/networks.html\n",
    "model = UNet(\n",
    "    # Image dimensions\n",
    "    spatial_dims= #CODE,\n",
    "    \n",
    "    # How many input channels?\n",
    "    in_channels= #CODE,\n",
    "    \n",
    "    # How many output channels? (That is, how many classes to recognize?)\n",
    "    out_channels= #CODE,\n",
    "    \n",
    "    # Feature maps at each level of the net\n",
    "    channels= #CODE,\n",
    "    \n",
    "    # Downsampling factors at each level\n",
    "    strides= #CODE,\n",
    "\n",
    "    # Number of residual blocks\n",
    "    num_res_units= #CODE,\n",
    "\n",
    "    # Normalization to be used  after convolutions\n",
    "    norm= #CODE,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:48:09.538734Z",
     "iopub.status.busy": "2025-11-27T10:48:09.538481Z",
     "iopub.status.idle": "2025-11-27T10:48:09.543708Z",
     "shell.execute_reply": "2025-11-27T10:48:09.5431Z",
     "shell.execute_reply.started": "2025-11-27T10:48:09.538711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from monai.metrics import #CODE\n",
    "from monai.losses import #CODE\n",
    "\n",
    "# Now we need to define a Loss Function: see https://monai.readthedocs.io/en/1.4.0/losses.html\n",
    "loss_function = #CODE\n",
    "\n",
    "# Optimizer...\n",
    "optimizer = #CODE\n",
    "\n",
    "# ...and here we define a suitble metric to evaluate the segmentation: see https://monai-dev.readthedocs.io/en/fixes-sphinx/metrics.html\n",
    "dice_metric = #CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:49:36.223904Z",
     "iopub.status.busy": "2025-11-27T10:49:36.223273Z",
     "iopub.status.idle": "2025-11-27T10:49:36.229713Z",
     "shell.execute_reply": "2025-11-27T10:49:36.229085Z",
     "shell.execute_reply.started": "2025-11-27T10:49:36.223881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We need to split our dataset_files in train and validation files\n",
    "train_files, val_files = #CODE\n",
    "print(len(train_files), len(val_files))\n",
    "\n",
    "# Now, since we created a specific SegmentationDataset class, let's use it\n",
    "# we need to extract the data from train and validation files applying transformations;\n",
    "# then we need to load the data with DataLoader\n",
    "train_dataset= #CODE\n",
    "train_loader = #CODE\n",
    "\n",
    "\n",
    "val_ds = #CODE\n",
    "val_loader = #CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:49:38.376196Z",
     "iopub.status.busy": "2025-11-27T10:49:38.375588Z",
     "iopub.status.idle": "2025-11-27T10:49:38.379779Z",
     "shell.execute_reply": "2025-11-27T10:49:38.379099Z",
     "shell.execute_reply.started": "2025-11-27T10:49:38.376174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We will need to save our model, so let's create our checkpoint_directory\n",
    "checkpoint_dir = BASE_DIR / \"models\"\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T15:37:14.872858Z",
     "iopub.status.busy": "2025-11-28T15:37:14.872224Z",
     "iopub.status.idle": "2025-11-28T15:37:14.877701Z",
     "shell.execute_reply": "2025-11-28T15:37:14.876903Z",
     "shell.execute_reply.started": "2025-11-28T15:37:14.872818Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Finally, the training loop\n",
    "\n",
    "max_epochs = #CODE\n",
    "val_interval = 1\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "#CODE\n",
    "#CODE\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n",
    "    #CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T10:54:25.567822Z",
     "iopub.status.busy": "2025-11-27T10:54:25.567593Z",
     "iopub.status.idle": "2025-11-27T10:54:25.825455Z",
     "shell.execute_reply": "2025-11-27T10:54:25.824689Z",
     "shell.execute_reply.started": "2025-11-27T10:54:25.567801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-28T15:43:00.462780Z",
     "iopub.status.idle": "2025-11-28T15:43:00.463065Z",
     "shell.execute_reply": "2025-11-28T15:43:00.462938Z",
     "shell.execute_reply.started": "2025-11-28T15:43:00.462925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Here you can load the model and check whether the predicted mask superimposes\n",
    "# correctly with initial image\n",
    "#CODE\n",
    "#CODE\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "        #CODE\n",
    "\n",
    "        # Plot results\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.imshow()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.imshow()\n",
    "        plt.imshow()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Prediction Overlay\")\n",
    "        plt.imshow()\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if i == 2:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 576013,
     "sourceId": 3324348,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
